{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import library\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import base64\n",
        "from requests import post, get\n",
        "\n",
        "import json\n",
        "import time\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import math\n",
        "\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "input_file_system_name = 'datasets'\n",
        "storage_account_name = 'spotifyprojectadls'\n",
        "adls_path = f\"abfss://{input_file_system_name}@{storage_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Start the timer\n",
        "starter = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **1. Access to Spotify API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "client_id = 'abcdef1234'\n",
        "client_secret = 'abcdef1234'\n",
        "\n",
        "def get_token():\n",
        "    url = 'https://accounts.spotify.com/api/token'\n",
        "    \n",
        "    auth_string = client_id + ':' + client_secret\n",
        "    auth_bytes = auth_string.encode(\"utf-8\")\n",
        "    auth_base64 = str(base64.b64encode(auth_bytes), \"utf-8\")\n",
        "\n",
        "    headers = {\n",
        "        'content-type': 'application/x-www-form-urlencoded',\n",
        "        'Authorization': 'Basic ' + auth_base64\n",
        "    }\n",
        "\n",
        "    form = {\n",
        "        'grant_type': 'client_credentials'\n",
        "    }\n",
        "\n",
        "    result = post(url, headers=headers, data=form)\n",
        "    json_result = json.loads(result.content)\n",
        "    token = json_result['access_token']\n",
        "    return token\n",
        "\n",
        "def get_auth_header(token):\n",
        "    return {\"Authorization\": f\"Bearer {token}\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **2. Today Top 200 Track Update**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Scrape for top 200 track daily and add into the previous one\n",
        "top_200_track_daily = pd.read_parquet(adls_path + 'top_200_track_daily.parquet')\n",
        "\n",
        "url = 'https://kworb.net/spotify/country/global_daily.html'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html')\n",
        "col_data = soup.find_all('tr')\n",
        "\n",
        "for row in col_data[1:]:\n",
        "    row_data = row.find_all('td')\n",
        "\n",
        "    track_url = row_data[2].find_all('a')[1].get('href')\n",
        "    artist_url = row_data[2].find_all('a')[0].get('href')\n",
        "\n",
        "    daily_stream = row_data[-5].text.strip()\n",
        "    daily_stream = daily_stream.replace(\",\",\"\")\n",
        "    daily_stream = int(daily_stream)\n",
        "\n",
        "    total_stream = row_data[-1].text.strip()\n",
        "    total_stream = total_stream.replace(\",\",\"\")\n",
        "    total_stream = int(total_stream)\n",
        "\n",
        "    track_pattern = r'../track/([^_]+)\\.html'\n",
        "    track_match = re.search(track_pattern, track_url)\n",
        "    if track_match:\n",
        "        track_id = track_match.group(1)\n",
        "    else:\n",
        "        track_id = np.NaN\n",
        "\n",
        "    artist_pattern = r'../artist/([^_]+)\\.html'\n",
        "    artist_match = re.search(artist_pattern, artist_url)\n",
        "    if artist_match:\n",
        "        artist_id = artist_match.group(1)\n",
        "    else:\n",
        "        artist_id = np.NaN\n",
        "\n",
        "    now = datetime.now()\n",
        "    extract_date = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    individual_row_data = [track_id, artist_id, daily_stream, total_stream, extract_date]\n",
        "\n",
        "    length = len(top_200_track_daily)\n",
        "    top_200_track_daily.loc[length] = individual_row_data\n",
        "\n",
        "top_200_track_daily['Extract_Date'] = pd.to_datetime(top_200_track_daily['Extract_Date'])\n",
        "\n",
        "top_200_update_result = 'Top 200 Update - SUCCESSFUL'\n",
        "print(top_200_update_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **3. Artist Images and Followers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def search_for_artists(artist_ids, _th):\n",
        "    url = f\"https://api.spotify.com/v1/artists?ids={artist_ids}\"\n",
        "    token = get_token()\n",
        "    headers = get_auth_header(token)\n",
        "\n",
        "    response = get(url, headers=headers)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            json_result = response.json()  # use .json() method directly\n",
        "            artists = pd.json_normalize(json_result['artists'])\n",
        "            \n",
        "            # artist_image table\n",
        "            artist_info = artists[['id', 'name', 'images']]\n",
        "            artist_info.rename(columns = {'id': 'Artist_ID', 'name': 'Artist_Name', 'images': 'Artist_Image'}, inplace=True)\n",
        "            artist_info['Artist_Image'] = artist_info['Artist_Image'].apply(lambda x: x[0]['url'] if x else None)\n",
        "\n",
        "            # artist_follower table\n",
        "            artist_follower = artists[['id', 'followers.total']]\n",
        "            artist_follower.rename(columns = {'id': 'Artist_ID', 'followers.total': 'Followers'}, inplace=True)\n",
        "\n",
        "            now = datetime.now()\n",
        "            artist_follower['Extract_Date'] = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # artist_genres table\n",
        "            artist_genres = artists[['id', 'genres']]\n",
        "            artist_genres.rename(columns={'id': 'Artist_ID', 'genres': 'Genre'}, inplace=True)\n",
        "            \n",
        "            return artist_info, artist_follower, artist_genres\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f'Other reason: {e}')\n",
        "            # Returning None or empty DataFrame to maintain consistency in return type\n",
        "            return None, None, None\n",
        "\n",
        "    elif response.status_code == 429:\n",
        "        print('Too many requests')\n",
        "        # Returning None or empty DataFrame to maintain consistency in return type\n",
        "        return None, None, None\n",
        "\n",
        "    else:\n",
        "        print(f'Failure - HTTP status code: {response.status_code}')\n",
        "        # Returning None or empty DataFrame to maintain consistency in return type\n",
        "        return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Search for existing artist info\n",
        "artist_info = pd.read_parquet(adls_path + 'artist_info.parquet')\n",
        "existed_artists = artist_info['Artist_ID'].tolist()\n",
        "\n",
        "concat_artist_ids = ''\n",
        "count = 1\n",
        "\n",
        "artist_image_list = []\n",
        "artist_follower_list = []\n",
        "artist_genres_list = []\n",
        "batch = 50\n",
        "\n",
        "for artist_id in existed_artists:\n",
        "    if count % batch in range(1, batch):\n",
        "        concat_artist_ids = concat_artist_ids + ',' + str(artist_id)\n",
        "        if count == len(existed_artists):\n",
        "            batch_th = math.ceil(count / batch)\n",
        "            concat_artist_ids = concat_artist_ids[1:]\n",
        "            result = search_for_artists(concat_artist_ids, batch_th)\n",
        "            \n",
        "            if result is None:\n",
        "                artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Other reasons or too many requests'\n",
        "                print(artist_search_result)\n",
        "                break\n",
        "\n",
        "            updated_artist_image, updated_artist_follower, updated_genres = result\n",
        "            artist_image_list.append(updated_artist_image)\n",
        "            artist_follower_list.append(updated_artist_follower)\n",
        "            artist_genres_list.append(updated_genres)\n",
        "            \n",
        "            artist_search_result = f'Artist Search - SUCCESSFUL: {batch_th} batches'\n",
        "            print(artist_search_result)\n",
        "    else:\n",
        "        batch_th = math.floor(count / batch)\n",
        "        concat_artist_ids = concat_artist_ids + ',' + str(artist_id)\n",
        "        concat_artist_ids = concat_artist_ids[1:]\n",
        "        \n",
        "        result = search_for_artists(concat_artist_ids, batch_th)\n",
        "        \n",
        "        if result is None:\n",
        "            artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Other reasons or too many requests'\n",
        "            print(artist_search_result)\n",
        "            break\n",
        "        \n",
        "        updated_artist_image, updated_artist_follower, updated_genres = result\n",
        "        artist_image_list.append(updated_artist_image)\n",
        "        artist_follower_list.append(updated_artist_follower)\n",
        "        artist_genres_list.append(updated_genres)\n",
        "\n",
        "        artist_search_result = f'Artist Search - SUCCESSFUL: {batch_th} batches'\n",
        "        print(artist_search_result)\n",
        "\n",
        "        concat_artist_ids = ''\n",
        "\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Update artist images\n",
        "\n",
        "try:\n",
        "    artist_image_update = pd.concat(artist_image_list, ignore_index=True)\n",
        "\n",
        "    # Find artist with updated images\n",
        "    compare_image = pd.merge(artist_info, artist_image_update, on='Artist_ID', how='outer')\n",
        "\n",
        "    # Updated image dataframe if there is update\n",
        "    change_image_artist = compare_image[compare_image['Artist_Image_x'] != compare_image['Artist_Image_y']]\n",
        "\n",
        "    # Update image in artist_info df\n",
        "    if len(change_image_artist) > 0:\n",
        "\n",
        "        # Loop through all updated artists\n",
        "        for updated_artist_id in change_image_artist['Artist_ID'].tolist():\n",
        "            # Get index of updated the row and the updated image\n",
        "            updated_index = change_image_artist[change_image_artist['Artist_ID'] == updated_artist_id].index.tolist()[0]\n",
        "            updated_image = change_image_artist.loc[updated_index, 'Artist_Image_y']\n",
        "\n",
        "            # UPDATE THE IMAGES\n",
        "            artist_index = artist_info[artist_info['Artist_ID'] == updated_artist_id].index.tolist()[0]\n",
        "            artist_info.loc[artist_index, 'Artist_Image'] = updated_image\n",
        "\n",
        "        images_no = len(change_image_artist)\n",
        "        update_artist_image_result = f'Image Update - SUCCESSFUL: {images_no} images updated'\n",
        "        print(update_artist_image_result)\n",
        "\n",
        "    else:\n",
        "        update_artist_image_result = f'Image Update - SUCCESSFUL: No image updated'\n",
        "        print(update_artist_image_result)\n",
        "\n",
        "except:\n",
        "    update_artist_image_result = f'Image Update - FAILED: Too many requests'\n",
        "    print(update_artist_image_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Update Artist Follower\n",
        "artist_follower = pd.read_parquet(adls_path + 'artist_follower.parquet')\n",
        "\n",
        "try:\n",
        "    artist_follower_update = pd.concat(artist_follower_list, ignore_index=True)\n",
        "\n",
        "    # Change columns' names\n",
        "    artist_follower_update.dropna(inplace=True)\n",
        "    artist_follower_update['Followers'] = artist_follower_update['Followers'].astype(int)\n",
        "    artist_follower_update = pd.merge(artist_info, artist_follower_update, on='Artist_ID', how='inner')[['Artist_ID', 'Followers', 'Extract_Date']]\n",
        "        \n",
        "    artist_follower = pd.concat([artist_follower, artist_follower_update], axis=0)\n",
        "    update_artist_follower_result = f'Follower Update - SUCCESSFUL'\n",
        "    print(update_artist_follower_result)\n",
        "\n",
        "except:\n",
        "    update_artist_follower_result = f'Follower Update - FAILED: Too many requests'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "artist_follower.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **4. Genres**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Add new genre\n",
        "genres = pd.read_parquet(adls_path + 'genres.parquet')\n",
        "existed_genres = genres['Genre_Name'].tolist()\n",
        "\n",
        "try:\n",
        "    updated_artist_genres = pd.concat(artist_genres_list, ignore_index=True)\n",
        "    updated_artist_genres = updated_artist_genres.explode('Genre')\n",
        "    updated_artist_genres.dropna(inplace=True)\n",
        "    genres_list = list(set(updated_artist_genres['Genre']))\n",
        "\n",
        "    new_genres = []\n",
        "    for new_genre in genres_list:\n",
        "        if new_genre not in existed_genres:\n",
        "            new_genres.append(new_genre)\n",
        "\n",
        "    if len(new_genres) > 0:\n",
        "        for genre_name in new_genres:\n",
        "            new_genres_id = len(genres) + 1\n",
        "            new_genre_row = [new_genres_id, genre_name]\n",
        "            genres.loc[length] = new_genre_row\n",
        "\n",
        "        no_new_genre = len(new_genres)\n",
        "        add_genre_result = f'Genres Addition - SUCCESSFUL: {no_new_genre} genres added'\n",
        "        print(add_genre_result)\n",
        "\n",
        "    else:\n",
        "        add_genre_result = f'Genres Addition - SUCCESSFUL: No new genre'\n",
        "        print(add_genre_result)\n",
        "\n",
        "except:\n",
        "    add_genre_result = f'Genres Addition - FAILED: Too many requests'\n",
        "    print(add_genre_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Lookup genre code and artist\n",
        "artist_genres = pd.read_parquet(adls_path + 'artist_genres.parquet')\n",
        "\n",
        "merge_df = pd.merge(updated_artist_genres, genres, left_on='Genre', right_on='Genre_Name')\n",
        "artist_genres = merge_df[['Artist_ID', 'Genre_ID']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **5. New Artists**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# If there is new artist in top 200 track, add them into artist info and artist_genres\n",
        "new_artists = []\n",
        "\n",
        "for artist_id in list(set(top_200_track_daily['Lead_Artist_ID'])):\n",
        "    if artist_id not in existed_artists:\n",
        "        new_artists.append(artist_id)\n",
        "\n",
        "len(new_artists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "concat_artist_ids = ''\n",
        "count = 1\n",
        "\n",
        "batch = 50\n",
        "\n",
        "if len(new_artists) > 0:\n",
        "    for artist_id in new_artists:\n",
        "        if count % batch in range(1,batch):\n",
        "            concat_artist_ids = concat_artist_ids + ',' + str(artist_id)\n",
        "            if count == len(new_artists):\n",
        "                batch_th = math.ceil(count/batch)\n",
        "                concat_artist_ids = concat_artist_ids[1:]\n",
        "                if search_for_artists(concat_artist_ids, batch_th) == 'Failure - Too many requests':\n",
        "                    artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                    print(artist_search_result)\n",
        "                    break\n",
        "\n",
        "                elif search_for_artists(concat_artist_ids, batch_th) == 'Failure - Other reasons':\n",
        "                    artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                    print(artist_search_result)\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    new_artist_info, new_artist_follower, artist_genres = search_for_artists(concat_artist_ids, batch_th)\n",
        "                    artist_info = pd.concat([artist_info, new_artist_info], axis=0)\n",
        "                    artist_follower = pd.concat([artist_follower, new_artist_follower], axis=0)\n",
        "\n",
        "                    artist_search_result = f'Artist Addition - SUCCESSFUL: {len(new_artists)} Artists added'\n",
        "                    print(artist_search_result)\n",
        "                    \n",
        "        else:\n",
        "            batch_th = math.floor(count/batch)\n",
        "            concat_artist_ids = concat_artist_ids + ',' + str(artist_id)\n",
        "            concat_artist_ids = concat_artist_ids[1:]\n",
        "            \n",
        "            if search_for_artists(concat_artist_ids, batch_th) == 'Failure - Too many requests':\n",
        "                artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                print(artist_search_result)\n",
        "                break\n",
        "\n",
        "            elif search_for_artists(concat_artist_ids, batch_th) == 'Failure - Other reasons':\n",
        "                artist_search_result = f'Artist Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                print(artist_search_result)\n",
        "                break\n",
        "            \n",
        "            else:\n",
        "                new_artist_info, new_artist_follower, artist_genres = search_for_artists(concat_artist_ids, batch_th)\n",
        "                artist_info = pd.concat([artist_info, new_artist_info], axis=0)\n",
        "                artist_follower = pd.concat([artist_follower, new_artist_follower], axis=0)\n",
        "\n",
        "                artist_search_result = f'Artist Addition - SUCCESSFUL: {len(new_artists)} Artists added'\n",
        "                print(artist_search_result)\n",
        "\n",
        "            concat_artist_ids = ''\n",
        "\n",
        "        count += 1\n",
        "\n",
        "else:\n",
        "    artist_search_result = f'Artist Addition - SUCCESSFUL: No New Artist'\n",
        "    print(artist_search_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "artist_follower['Extract_Date'] = pd.to_datetime(artist_follower['Extract_Date'], format='%Y-%m-%d %H:%M:%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **6. Get New Song Stream**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "col_title = ['Song_ID', 'Stream', 'Extract_Date']\n",
        "song_stream_updated = pd.DataFrame(columns = col_title)\n",
        "\n",
        "for artist_id in artist_info['Artist_ID']:\n",
        "    pattern_url = f'https://kworb.net/spotify/artist/{artist_id}_songs.html'\n",
        "\n",
        "    page = requests.get(pattern_url)\n",
        "    soup = BeautifulSoup(page.text, 'html')\n",
        "\n",
        "    col_data = soup.find_all('tr')\n",
        "\n",
        "    for row in col_data[5:]:\n",
        "        row_data = row.find_all('td')\n",
        "        song_url = row_data[0].find_all('a')[0].get('href')\n",
        "        pattern = r'https://open.spotify.com/track/([^_]+)'\n",
        "        match = re.search(pattern, song_url)\n",
        "\n",
        "        song_id = match.group(1)\n",
        "        streams = [data.text.strip() for data in row_data][1]\n",
        "        \n",
        "        now = datetime.now()\n",
        "        extract_date = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        individual_row_data = [song_id, streams, extract_date]\n",
        "\n",
        "        length = len(song_stream_updated)\n",
        "        song_stream_updated.loc[length] = individual_row_data\n",
        "\n",
        "# Delete comma in stream number and convert into integer\n",
        "song_stream_updated['Stream'] = song_stream_updated['Stream'].apply(lambda x: x.replace(\",\",\"\"))\n",
        "song_stream_updated['Stream'] = song_stream_updated['Stream'].astype(int)\n",
        "\n",
        "# Convert Extract date into datetime type\n",
        "song_stream_updated['Extract_Date'] = pd.to_datetime(song_stream_updated['Extract_Date'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Delete duplicates\n",
        "song_stream_updated.drop_duplicates(subset='Song_ID', keep='first', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **7. New Song Info and Stream**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def search_for_tracks(track_ids, _th):\n",
        "    url = f\"https://api.spotify.com/v1/tracks?ids={track_ids}\"\n",
        "    token = get_token()\n",
        "    headers = get_auth_header(token)\n",
        "    response = get(url, headers=headers)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        \n",
        "        try:\n",
        "            json_result = response.json()  # use .json() method directly\n",
        "            songs = pd.json_normalize(json_result['tracks'])\n",
        "            \n",
        "            #song_info table\n",
        "            song_info = songs[['id', 'name', 'explicit', 'duration_ms', 'track_number', 'disc_number']]\n",
        "            song_info.rename(columns={'id': 'Song_ID',\n",
        "                                        'name': 'Song_Name',\n",
        "                                        'explicit': 'Explicit',\n",
        "                                        'duration_ms': 'Duration',\n",
        "                                        'track_number': 'Track_Number',\n",
        "                                        'disc_number': 'Disc_Number'},\n",
        "                                        inplace=True)\n",
        "\n",
        "            #artist_song table\n",
        "            artist_song = songs[['id', 'artists', 'album.artists']]\n",
        "            artist_song.rename(columns={'id': 'Song_ID', 'artists': 'Artist_ID', 'album.artists': 'Artist_ID_Album'}, inplace=True)\n",
        "            \n",
        "            #album_song table\n",
        "            album_song = songs[['id', 'album.id']]\n",
        "            album_song.rename(columns={'id': 'Song_ID', 'album.id': 'Album_ID'}, inplace=True)\n",
        "\n",
        "            return song_info, artist_song, album_song\n",
        "        \n",
        "        except:\n",
        "            other_reason = 'Failure - Other reasons'\n",
        "            print('Other reasons')\n",
        "            return other_reason\n",
        "    \n",
        "    else:\n",
        "        failure_retry_after = 'Failure - Too many requests'\n",
        "        print('Too many requests')\n",
        "        return failure_retry_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "song_info = pd.read_parquet(adls_path + 'song_info.parquet')\n",
        "song_stream = pd.read_parquet(adls_path + 'song_stream.parquet')\n",
        "\n",
        "merge_df_1 = pd.merge(song_stream_updated, song_stream, on='Song_ID', how='left')\n",
        "new_tracks_1 = merge_df_1[merge_df_1.isnull().any(axis=1)]['Song_ID'].tolist()\n",
        "\n",
        "merge_df_2 = pd.merge(top_200_track_daily, song_stream, on='Song_ID', how='left')\n",
        "new_tracks_2 = merge_df_2[merge_df_2.isnull().any(axis=1)]['Song_ID'].tolist()\n",
        "\n",
        "new_tracks = new_tracks_1 + new_tracks_2\n",
        "\n",
        "if len(new_tracks) > 0:\n",
        "    concat_song_ids = ''\n",
        "    count = 1\n",
        "\n",
        "    song_info_list = []\n",
        "    artist_song_list = []\n",
        "    album_song_list = []\n",
        "    batch = 50\n",
        "\n",
        "    for song_id in new_tracks:\n",
        "        if count % batch in range(1,batch):\n",
        "            concat_song_ids = concat_song_ids + ',' + str(song_id)\n",
        "            if count == len(new_tracks):\n",
        "                batch_th = math.ceil(count/batch)\n",
        "                concat_song_ids = concat_song_ids[1:]\n",
        "                if search_for_tracks(concat_song_ids, batch_th) == 'Failure - Too many requests':\n",
        "                    track_search_result = f'Track Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                    print(track_search_result)\n",
        "                    break\n",
        "\n",
        "                elif search_for_tracks(concat_song_ids, batch_th) == 'Failure - Other reasons':\n",
        "                    track_search_result = f'Track Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                    print(track_search_result)\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    added_song_info, artist_song, album_song = search_for_tracks(concat_song_ids, batch_th)\n",
        "                    song_info = pd.concat([song_info, added_song_info], axis=0)\n",
        "\n",
        "                    artist_song_list.append(artist_song)\n",
        "                    album_song_list.append(album_song)\n",
        "                    \n",
        "                    track_search_result = f'Track Search - SUCCESSFUL: {batch_th} batches'\n",
        "                    track_addition_result = f'Track Addition - SUCCESSFUL: {len(new_tracks)} tracks added'\n",
        "                    print(track_search_result)\n",
        "                    print(track_addition_result)\n",
        "        else:\n",
        "            batch_th = math.floor(count/batch)\n",
        "            concat_song_ids = concat_song_ids + ',' + str(song_id)\n",
        "            concat_song_ids = concat_song_ids[1:]\n",
        "            \n",
        "            if search_for_tracks(concat_song_ids, batch_th) == 'Failure - Too many requests':\n",
        "                track_search_result = f'Track Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                print(track_search_result)\n",
        "                break\n",
        "            \n",
        "            elif search_for_tracks(concat_song_ids, batch_th) == 'Failure - Other reasons':\n",
        "                track_search_result = f'Track Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                print(track_search_result)\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                added_song_info, artist_song, album_song = search_for_tracks(concat_song_ids, batch_th)\n",
        "                song_info = pd.concat([song_info, added_song_info], axis=0)\n",
        "                \n",
        "                artist_song_list.append(artist_song)\n",
        "                album_song_list.append(album_song)\n",
        "                \n",
        "                track_search_result = f'Track Search - SUCCESSFUL: {batch_th} batches'\n",
        "                track_addition_result = f'Track Addition - SUCCESSFUL: {len(new_tracks)} tracks added'\n",
        "                print(track_search_result)\n",
        "                print(track_addition_result)\n",
        "\n",
        "            concat_song_ids = ''\n",
        "\n",
        "        count += 1\n",
        "else:\n",
        "    track_search_result = f'Track Search - SUCCESSFUL: No track updated'\n",
        "    print(track_search_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "song_info.drop_duplicates(subset='Song_ID', inplace=True)\n",
        "song_stream = pd.concat([song_stream, song_stream_updated], axis=0)\n",
        "song_stream_update_result = f'Stream Update - SUCCESSFUL'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **8. Lead and Feature Artist**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Lead Artist\n",
        "    added_artist_song = pd.concat(artist_song_list, ignore_index=True)\n",
        "\n",
        "    def extract_ids(artists):\n",
        "        return [artist['id'] for artist in artists]\n",
        "\n",
        "    added_artist_song['Artist_ID'] = added_artist_song['Artist_ID'].apply(extract_ids)\n",
        "    added_artist_song['Artist_ID_Album'] = added_artist_song['Artist_ID_Album'].apply(extract_ids)\n",
        "\n",
        "    def find_lead_artists(row):\n",
        "        return [artist for artist in row['Artist_ID'] if artist in row['Artist_ID_Album']]\n",
        "\n",
        "    added_artist_song['Lead_Artist_ID'] = added_artist_song.apply(find_lead_artists, axis=1)\n",
        "\n",
        "    def find_feature_artists(row):\n",
        "        return [artist for artist in row['Artist_ID'] if artist not in row['Artist_ID_Album']]\n",
        "\n",
        "    added_artist_song['Feature_Artist_ID'] = added_artist_song.apply(find_feature_artists, axis=1)\n",
        "\n",
        "    added_lead_artist = added_artist_song[['Lead_Artist_ID', 'Song_ID']]\n",
        "    added_lead_artist = added_lead_artist.explode('Lead_Artist_ID')\n",
        "    added_lead_artist.drop_duplicates(inplace=True)\n",
        "\n",
        "    merge_df = pd.merge(added_lead_artist, artist_info, left_on='Lead_Artist_ID',\n",
        "                        right_on='Artist_ID', how='left')\n",
        "    no_lead_artist_rows = merge_df[merge_df['Lead_Artist_ID'].isnull()]['Song_ID'].tolist()\n",
        "    added_artist_song['Lead_Artist_ID'] = added_artist_song.apply(lambda x: x['Artist_ID'] if x['Song_ID'] in no_lead_artist_rows\n",
        "                                                else x['Lead_Artist_ID'], axis=1)\n",
        "    added_artist_song['Feature_Artist_ID'] = added_artist_song.apply(lambda x: [] if x['Song_ID'] in no_lead_artist_rows\n",
        "                                                    else x['Feature_Artist_ID'], axis=1)\n",
        "    added_lead_artist = added_artist_song[['Lead_Artist_ID', 'Song_ID']]\n",
        "    added_lead_artist = added_lead_artist.explode('Lead_Artist_ID')\n",
        "    added_lead_artist.drop_duplicates(inplace=True)\n",
        "\n",
        "    merge_df = pd.merge(added_lead_artist, artist_info, left_on='Lead_Artist_ID',\n",
        "                        right_on='Artist_ID', how='left')\n",
        "    merge_df.dropna(inplace=True)\n",
        "\n",
        "    added_lead_artist = merge_df[['Lead_Artist_ID', 'Song_ID']]\n",
        "\n",
        "    \n",
        "\n",
        "    # Feature Artist\n",
        "    added_feature_artist = added_artist_song[['Feature_Artist_ID', 'Song_ID']]\n",
        "    added_feature_artist = added_feature_artist.explode('Feature_Artist_ID')\n",
        "    added_feature_artist.drop_duplicates(inplace=True)\n",
        "\n",
        "    merge_df = pd.merge(added_feature_artist, artist_info, left_on='Feature_Artist_ID',\n",
        "                        right_on='Artist_ID', how='left')\n",
        "    merge_df.dropna(inplace=True)\n",
        "\n",
        "    added_feature_artist = merge_df[['Feature_Artist_ID', 'Song_ID']]\n",
        "    added_feature_artist = added_feature_artist.reset_index(drop=True)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "except:\n",
        "    lead_feature_artist_addition_result = 'Lead-Feature Artist Addition - SUCCESSFULL: No artist added'\n",
        "    print(lead_feature_artist_addition_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Add into the lead and feature artist\n",
        "lead_artist = pd.read_parquet(adls_path + 'lead_artist.parquet')\n",
        "feature_artist = pd.read_parquet(adls_path + 'feature_artist.parquet')\n",
        "\n",
        "try:\n",
        "    lead_artist = pd.concat([lead_artist, added_lead_artist], axis=0)\n",
        "    feature_artist = pd.concat([feature_artist, added_feature_artist], axis=0)\n",
        "\n",
        "    lead_feature_artist_addition_result = 'Lead-Feature Artist Addition - SUCCESSFULL'\n",
        "    print(lead_feature_artist_addition_result)\n",
        "\n",
        "except:\n",
        "    lead_feature_artist_addition_result = 'Lead-Feature Artist Addition - SUCCESSFULL: No artist added'\n",
        "    print(lead_feature_artist_addition_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **9. New Album**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def search_for_albums(album_ids, max_retries=5, backoff_factor=1.0):\n",
        "    url = f\"https://api.spotify.com/v1/albums?ids={album_ids}\"\n",
        "    token = get_token()\n",
        "    headers = get_auth_header(token)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        response = get(url, headers=headers)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            \n",
        "            json_result = response.json()  # use .json() method directly\n",
        "            albums = pd.json_normalize(json_result['albums'])\n",
        "            \n",
        "            #album_info table\n",
        "            album_info = albums[['id', 'name', 'total_tracks', 'album_type',\n",
        "                                 'release_date', 'images', 'label']]\n",
        "            try:\n",
        "                album_info['images'] = album_info['images'].apply(lambda x: x[0]['url'])\n",
        "            except:\n",
        "                album_info['images'] = np.NaN\n",
        "            album_info.rename(columns={'id': 'Album_ID',\n",
        "                                        'name': 'Album_Name',\n",
        "                                        'total_tracks': 'Total_Tracks',\n",
        "                                        'album_type': 'Album_Type',\n",
        "                                        'release_date': 'Release_Date',\n",
        "                                        'images': 'Album_Image',\n",
        "                                        'label': 'Label'},\n",
        "                                        inplace=True)\n",
        "\n",
        "            #artist_album table\n",
        "            artist_album = albums[['id', 'artists']]\n",
        "            artist_album['artists'] = artist_album['artists'].apply(extract_ids)\n",
        "            artist_album = artist_album.explode('artists')\n",
        "            artist_album.drop_duplicates(inplace=True)\n",
        "            artist_album.rename(columns={'id': 'Album_ID',\n",
        "                                        'artists': 'Artist_ID'}, inplace=True)\n",
        "\n",
        "            return album_info, artist_album\n",
        "        \n",
        "        elif response.status_code == 429:\n",
        "            retry_after = int(response.headers.get(\"Retry-After\", backoff_factor * (2 ** attempt)))\n",
        "            print(f\"Too many requests. Retrying in {retry_after} seconds...\")\n",
        "            time.sleep(retry_after)\n",
        "        \n",
        "        else:\n",
        "            response.raise_for_status()\n",
        "    \n",
        "    raise Exception(\"Maximum retries exceeded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    added_adlbum_song = pd.concat(album_song_list, ignore_index=True)\n",
        "    new_album = list(set(added_adlbum_song['Album_ID']))\n",
        "\n",
        "    concat_album_ids = ''\n",
        "    count = 1\n",
        "\n",
        "    album_info_list = []\n",
        "    artist_album_list = []\n",
        "    batch = 20\n",
        "\n",
        "    for album_id in new_album:\n",
        "        if count % batch in range(1,batch):\n",
        "            concat_album_ids = concat_album_ids + ',' + str(album_id)\n",
        "            if count == len(new_album):\n",
        "                batch_th = math.ceil(count/batch)\n",
        "                concat_album_ids = concat_album_ids[1:]\n",
        "                if search_for_albums(concat_album_ids, batch_th) == 'Failure - Too many requests':\n",
        "                    album_search_result = f'Album Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                    print(album_search_result)\n",
        "                    break\n",
        "\n",
        "                elif search_for_albums(concat_album_ids, batch_th) == 'Failure - Other reasons':\n",
        "                    album_search_result = f'Album Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                    print(album_search_result)\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    new_album_info, new_artist_album = search_for_albums(concat_album_ids, batch_th)\n",
        "                    album_info_list.append(new_album_info)\n",
        "                    artist_album_list.append(new_artist_album)\n",
        "                    \n",
        "                    album_search_result = f'Album Search - SUCCESSFUL: {batch_th} batches'\n",
        "                    print(album_search_result)\n",
        "\n",
        "        else:\n",
        "            batch_th = math.floor(count/batch)\n",
        "            concat_album_ids = concat_album_ids + ',' + str(album_id)\n",
        "            concat_album_ids = concat_album_ids[1:]\n",
        "            \n",
        "            if search_for_albums(concat_album_ids, batch_th) == 'Failure - Too many requests':\n",
        "                album_search_result = f'Album Search - FAILED: Batch {batch_th} - Too many requests'\n",
        "                print(album_search_result)\n",
        "                break\n",
        "\n",
        "            elif search_for_albums(concat_album_ids, batch_th) == 'Failure - Other reasons':\n",
        "                album_search_result = f'Album Search - FAILED: Batch {batch_th} - Other reasons'\n",
        "                print(album_search_result)\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                new_album_info, new_artist_album = search_for_albums(concat_album_ids, batch_th)\n",
        "                album_info_list.append(new_album_info)\n",
        "                artist_album_list.append(new_artist_album)\n",
        "                \n",
        "                album_search_result = f'Album Search - SUCCESSFUL: {batch_th} batches'\n",
        "                print(album_search_result)\n",
        "\n",
        "            concat_album_ids = ''\n",
        "\n",
        "        count += 1\n",
        "        \n",
        "except:\n",
        "    album_search_result = f'Album Search - SUCCESSFUL: No album added'\n",
        "    print(album_search_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Add new album into album info and artist album\n",
        "album_info = pd.read_parquet(adls_path + 'album_info.parquet')\n",
        "artist_album = pd.read_parquet(adls_path + 'artist_album.parquet')\n",
        "album_song = pd.read_parquet(adls_path + 'album_song.parquet')\n",
        "\n",
        "try:\n",
        "    no_new_album = len(new_album)\n",
        "    new_album_info = pd.concat(album_info_list, ignore_index=True)\n",
        "    new_artist_album = pd.concat(artist_album_list, ignore_index=True)\n",
        "    new_album_song = pd.concat(album_song_list, ignore_index=True)\n",
        "\n",
        "    album_info = pd.concat([album_info, new_album_info], axis=0)\n",
        "    artist_album = pd.concat([artist_album, new_artist_album], axis=0)\n",
        "    album_song = pd.concat([album_song, new_album_song], axis=0)\n",
        "\n",
        "    album_addition_result = f'Album Addition - SUCCESSFUL: {no_new_album} albums added'\n",
        "    print(album_addition_result)\n",
        "\n",
        "except:\n",
        "    album_addition_result = f'Album Addition - SUCCESSFUL: No new album'\n",
        "    print(album_addition_result)\n",
        "\n",
        "album_info['Release_Date'] = pd.to_datetime(album_info['Release_Date'])\n",
        "album_info.drop_duplicates(subset='Album_ID', inplace=True)\n",
        "album_info = album_info.reset_index(drop=True)\n",
        "artist_album = artist_album.reset_index(drop=True)\n",
        "album_song = album_song.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **10. Remove Same Song but Different IDs in song_stream**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "song_stream['Date'] = song_stream['Extract_Date'].dt.date\n",
        "merge_df = pd.merge(song_stream, song_info, on='Song_ID', how='left')[['Song_ID', 'Song_Name', 'Stream', 'Extract_Date', 'Date']]\n",
        "merge_df.drop_duplicates(subset=['Song_Name', 'Stream', 'Date'], inplace=True)\n",
        "merge_df = merge_df.reset_index(drop=True)\n",
        "song_stream = merge_df[['Song_ID', 'Stream', 'Extract_Date']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Exporting Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "output_file_system_name = 'testing'\n",
        "storage_account_name = 'spotifyprojectadls'\n",
        "adls_path = f\"abfss://{output_file_system_name}@{storage_account_name}.dfs.core.windows.net/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "top_200_track_daily.to_parquet(adls_path + 'top_200_track_daily.parquet', index=False)\n",
        "song_stream.to_parquet(adls_path + 'song_stream.parquet', index=False)\n",
        "artist_follower.to_parquet(adls_path + 'artist_follower.parquet', index=False)\n",
        "artist_info.to_parquet(adls_path + 'artist_info.parquet', index=False)\n",
        "album_info.to_parquet(adls_path + 'album_info.parquet', index=False)\n",
        "artist_genres.to_parquet(adls_path + 'artist_genres.parquet', index=False)\n",
        "genres.to_parquet(adls_path + 'genres.parquet', index=False)\n",
        "lead_artist.to_parquet(adls_path + 'lead_artist.parquet', index=False)\n",
        "feature_artist.to_parquet(adls_path + 'feature_artist.parquet', index=False)\n",
        "album_song.to_parquet(adls_path + 'album_song.parquet', index=False)\n",
        "artist_album.to_parquet(adls_path + 'artist_album.parquet', index=False)\n",
        "song_info.to_parquet(adls_path + 'song_info.parquet', index=False)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
